{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Cluster \u00b6 This repository is my home Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a Github issue or join the k8s@home Discord if you have any questions. This repository is built off the k8s-at-home/template-cluster-k3s repository. Cluster setup \u00b6 My cluster is k3s provisioned overtop Ubuntu 21.04 using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes while I have a separate server for (NFS) file storage. See my ansible directory for my playbooks and roles. Cluster components \u00b6 rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository. kube-vip : HA solution for Kubernetes control plane Kasten : Data backup and recovery Repository structure \u00b6 The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. base directory is the entrypoint to Flux crds directory contains custom resource definitions (CRDs) that need to exist globally in your cluster before anything else exists core directory (depends on crds ) are important infrastructure applications (grouped by namespace) that should never be pruned by Flux apps directory (depends on core ) is where your common applications (grouped by namespace) could be placed, Flux will prune resources here if they are not tracked by Git anymore ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u2514\u2500\u2500 ./crds Automate all the things! \u00b6 Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date Hardware \u00b6 Device Count OS Disk Size Data Disk Size Ram Purpose CP-01 1 50GB NVMe 50GB NVMe (rook-ceph) 8GB k3s Masters (embedded etcd) WK-[1-2] 2 50GB NVMe 50GB NVMe (rook-ceph) 12GB k3s Workers Tools \u00b6 | Tool | Purpose | | ------------------------------------------------------ | ------------------------------------------------------------ | | | pre-commit | Enforce code consistency and verifies no secrets are pushed | | stern | Tail logs in Kubernetes | Thanks \u00b6 A lot of inspiration for my cluster came from the people that have shared their clusters over at awesome-home-kubernetes","title":"Introduction"},{"location":"#home-cluster","text":"This repository is my home Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a Github issue or join the k8s@home Discord if you have any questions. This repository is built off the k8s-at-home/template-cluster-k3s repository.","title":"Home Cluster"},{"location":"#cluster-setup","text":"My cluster is k3s provisioned overtop Ubuntu 21.04 using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes while I have a separate server for (NFS) file storage. See my ansible directory for my playbooks and roles.","title":"Cluster setup"},{"location":"#cluster-components","text":"rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository. kube-vip : HA solution for Kubernetes control plane Kasten : Data backup and recovery","title":"Cluster components"},{"location":"#repository-structure","text":"The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. base directory is the entrypoint to Flux crds directory contains custom resource definitions (CRDs) that need to exist globally in your cluster before anything else exists core directory (depends on crds ) are important infrastructure applications (grouped by namespace) that should never be pruned by Flux apps directory (depends on core ) is where your common applications (grouped by namespace) could be placed, Flux will prune resources here if they are not tracked by Git anymore ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u2514\u2500\u2500 ./crds","title":"Repository structure"},{"location":"#automate-all-the-things","text":"Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date","title":"Automate all the things!"},{"location":"#hardware","text":"Device Count OS Disk Size Data Disk Size Ram Purpose CP-01 1 50GB NVMe 50GB NVMe (rook-ceph) 8GB k3s Masters (embedded etcd) WK-[1-2] 2 50GB NVMe 50GB NVMe (rook-ceph) 12GB k3s Workers","title":"Hardware"},{"location":"#tools","text":"| Tool | Purpose | | ------------------------------------------------------ | ------------------------------------------------------------ | | | pre-commit | Enforce code consistency and verifies no secrets are pushed | | stern | Tail logs in Kubernetes |","title":"Tools"},{"location":"#thanks","text":"A lot of inspiration for my cluster came from the people that have shared their clusters over at awesome-home-kubernetes","title":"Thanks"},{"location":"_archived/kasten-data-backup/","text":"Kasten Data Backup \u00b6 Work in progress This document is a work in progress.","title":"Kasten Data Backup"},{"location":"_archived/kasten-data-backup/#kasten-data-backup","text":"Work in progress This document is a work in progress.","title":"Kasten Data Backup"},{"location":"_archived/kasten-data-restore/","text":"Kasten Data Restore \u00b6 Recovering from a K10 backup involves the following sequence of actions Create k10-dr-secret Kubernetes Secret \u00b6 The <passphrase> was set during the first installation of k10 kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key=<passphrase> Install a fresh K10 instance \u00b6 Ensure that Flux has correctly deployed K10 to it's namespace kasten-io flux get hr -n kasten-io Verify the nfs storage profile was created \u00b6 kubectl get profiles -n kasten-io Restoring the K10 backup \u00b6 Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job The <source-cluster-id> was set during the first installation of k10 helm install k10-restore kasten/k10restore -n kasten-io \\ --set sourceClusterID=<source-cluster-id> \\ --set profile.name=nfs Application recovery \u00b6 Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Kasten Data Restore"},{"location":"_archived/kasten-data-restore/#kasten-data-restore","text":"Recovering from a K10 backup involves the following sequence of actions","title":"Kasten Data Restore"},{"location":"_archived/kasten-data-restore/#create-k10-dr-secret-kubernetes-secret","text":"The <passphrase> was set during the first installation of k10 kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key=<passphrase>","title":"Create k10-dr-secret Kubernetes Secret"},{"location":"_archived/kasten-data-restore/#install-a-fresh-k10-instance","text":"Ensure that Flux has correctly deployed K10 to it's namespace kasten-io flux get hr -n kasten-io","title":"Install a fresh K10 instance"},{"location":"_archived/kasten-data-restore/#verify-the-nfs-storage-profile-was-created","text":"kubectl get profiles -n kasten-io","title":"Verify the nfs storage profile was created"},{"location":"_archived/kasten-data-restore/#restoring-the-k10-backup","text":"Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job The <source-cluster-id> was set during the first installation of k10 helm install k10-restore kasten/k10restore -n kasten-io \\ --set sourceClusterID=<source-cluster-id> \\ --set profile.name=nfs","title":"Restoring the K10 backup"},{"location":"_archived/kasten-data-restore/#application-recovery","text":"Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Application recovery"},{"location":"_archived/snmp-exporter/","text":"SNMP Exporter \u00b6 Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus Clone and build the snmp-exporter generator \u00b6 sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${GOPATH-$HOME/go}/src/github.com/prometheus/snmp_exporter/generator go build make mibs Update generator.yml \u00b6 Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules: apcups: version: 1 walk: - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups: - source_indexes: [upsOutletGroupStatusIndex] lookup: upsOutletGroupStatusName drop_source_indexes: true - source_indexes: [iemStatusProbeIndex] lookup: iemStatusProbeName drop_source_indexes: true overrides: ifType: type: EnumAsInfo rPDU2BankStatusLoadState: type: EnumAsStateSet upsAdvBatteryCondition: type: EnumAsStateSet upsAdvBatteryChargingCurrentRestricted: type: EnumAsStateSet upsAdvBatteryChargerStatus: type: EnumAsStateSet cyberpower: version: 1 walk: - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%) Get the Cyberpower MIB \u00b6 wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/ Generate the snmp.yml \u00b6 This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS=mibs ./generator generate","title":"SNMP Exporter"},{"location":"_archived/snmp-exporter/#snmp-exporter","text":"Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus","title":"SNMP Exporter"},{"location":"_archived/snmp-exporter/#clone-and-build-the-snmp-exporter-generator","text":"sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${GOPATH-$HOME/go}/src/github.com/prometheus/snmp_exporter/generator go build make mibs","title":"Clone and build the snmp-exporter generator"},{"location":"_archived/snmp-exporter/#update-generatoryml","text":"Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules: apcups: version: 1 walk: - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups: - source_indexes: [upsOutletGroupStatusIndex] lookup: upsOutletGroupStatusName drop_source_indexes: true - source_indexes: [iemStatusProbeIndex] lookup: iemStatusProbeName drop_source_indexes: true overrides: ifType: type: EnumAsInfo rPDU2BankStatusLoadState: type: EnumAsStateSet upsAdvBatteryCondition: type: EnumAsStateSet upsAdvBatteryChargingCurrentRestricted: type: EnumAsStateSet upsAdvBatteryChargerStatus: type: EnumAsStateSet cyberpower: version: 1 walk: - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%)","title":"Update generator.yml"},{"location":"_archived/snmp-exporter/#get-the-cyberpower-mib","text":"wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/","title":"Get the Cyberpower MIB"},{"location":"_archived/snmp-exporter/#generate-the-snmpyml","text":"This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS=mibs ./generator generate","title":"Generate the snmp.yml"},{"location":"_archived/velero/","text":"Velero \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install velero Create a backup \u00b6 Create a backup for all apps: velero backup create manually-backup-1 --from-schedule velero-daily-backup Create a backup for a single app: velero backup create jackett-test-abc \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait Delete resources \u00b6 Delete the HelmRelease : kubectl delete hr jackett-test -n testing Wait Allow the application to be redeployed and create the new resources Delete the new resources: kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config Restore \u00b6 velero restore create \\ --from-backup velero-daily-backup-20201120020022 \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Velero"},{"location":"_archived/velero/#velero","text":"Work in progress This document is a work in progress.","title":"Velero"},{"location":"_archived/velero/#install-the-cli-tool","text":"brew install velero","title":"Install the CLI tool"},{"location":"_archived/velero/#create-a-backup","text":"Create a backup for all apps: velero backup create manually-backup-1 --from-schedule velero-daily-backup Create a backup for a single app: velero backup create jackett-test-abc \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Create a backup"},{"location":"_archived/velero/#delete-resources","text":"Delete the HelmRelease : kubectl delete hr jackett-test -n testing Wait Allow the application to be redeployed and create the new resources Delete the new resources: kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config","title":"Delete resources"},{"location":"_archived/velero/#restore","text":"velero restore create \\ --from-backup velero-daily-backup-20201120020022 \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Restore"},{"location":"applications/paperless/","text":"Paperless user creation \u00b6 To create the first superuser for you need to execute the following command in the paperless-container su paperless cd /usr/src/paperless/src manage.py createsuperuser","title":"Paperless"},{"location":"applications/paperless/#paperless-user-creation","text":"To create the first superuser for you need to execute the following command in the paperless-container su paperless cd /usr/src/paperless/src manage.py createsuperuser","title":"Paperless user creation"},{"location":"installation/bootstrap-applications/","text":"Bootstrapping Applications \u00b6 The Kubernetes @Home community has created a wonderful Github template for bootstrapping a cluster for flux this can be viewed here . Bootstrapping Flux \u00b6 Create or locate cluster GPG key \u00b6 export GPG_TTY=$(tty) gpg --list-secret-keys \"Home cluster (Flux) <email>\" export FLUX_KEY_FP=ABCDEFGHIJKLMNOPQRSTUVWXYZ Verify cluster is ready for Flux \u00b6 flux --kubeconfig=./kubeconfig check --pre Pre-create the flux-system namespace \u00b6 kubectl --kubeconfig=./kubeconfig create namespace flux-system --dry-run=client -o yaml | kubectl --kubeconfig=./kubeconfig apply -f - Add the Flux GPG key in-order for Flux to decrypt SOPS secrets \u00b6 gpg --export-secret-keys --armor \"${FLUX_KEY_FP}\" | kubectl --kubeconfig=./kubeconfig create secret generic sops-gpg \\ --namespace=flux-system \\ --from-file=sops.asc=/dev/stdin Install Flux \u00b6 Due to race conditions with the Flux CRDs you will have to run the below command twice. There should be no errors on this second run. kubectl --kubeconfig=./kubeconfig apply --kustomize=./cluster/base/flux-system :tada: at this point after reconciliation Flux state should be restored.","title":"Bootstrap Applications"},{"location":"installation/bootstrap-applications/#bootstrapping-applications","text":"The Kubernetes @Home community has created a wonderful Github template for bootstrapping a cluster for flux this can be viewed here .","title":"Bootstrapping Applications"},{"location":"installation/bootstrap-applications/#bootstrapping-flux","text":"","title":"Bootstrapping Flux"},{"location":"installation/bootstrap-applications/#create-or-locate-cluster-gpg-key","text":"export GPG_TTY=$(tty) gpg --list-secret-keys \"Home cluster (Flux) <email>\" export FLUX_KEY_FP=ABCDEFGHIJKLMNOPQRSTUVWXYZ","title":"Create or locate cluster GPG key"},{"location":"installation/bootstrap-applications/#verify-cluster-is-ready-for-flux","text":"flux --kubeconfig=./kubeconfig check --pre","title":"Verify cluster is ready for Flux"},{"location":"installation/bootstrap-applications/#pre-create-the-flux-system-namespace","text":"kubectl --kubeconfig=./kubeconfig create namespace flux-system --dry-run=client -o yaml | kubectl --kubeconfig=./kubeconfig apply -f -","title":"Pre-create the flux-system namespace"},{"location":"installation/bootstrap-applications/#add-the-flux-gpg-key-in-order-for-flux-to-decrypt-sops-secrets","text":"gpg --export-secret-keys --armor \"${FLUX_KEY_FP}\" | kubectl --kubeconfig=./kubeconfig create secret generic sops-gpg \\ --namespace=flux-system \\ --from-file=sops.asc=/dev/stdin","title":"Add the Flux GPG key in-order for Flux to decrypt SOPS secrets"},{"location":"installation/bootstrap-applications/#install-flux","text":"Due to race conditions with the Flux CRDs you will have to run the below command twice. There should be no errors on this second run. kubectl --kubeconfig=./kubeconfig apply --kustomize=./cluster/base/flux-system :tada: at this point after reconciliation Flux state should be restored.","title":"Install Flux"},{"location":"installation/guidelines/","text":"Guidelines \u00b6 Here are several suggestions I have prior to installing Kubernetes or just general use. Some of these suggestions may only apply to Ubuntu. Storage \u00b6 Do not use NFS for application configuration data if that application uses SQLite with write ahead logging (WAL), or uses file locks. Applications like Sonarr, Radarr, Lidarr are clear examples to avoid using NFS for the configuration volume. Understand the importance, capabilities and limitations between file, block and object type storage. Networking \u00b6 Configure DNS on your nodes to use an upstream provider (e.g. 1.1.1.1 , 9.9.9.9 ), or your router's IP if you have DNS configured there and it's not pointing to a local adblocker DNS . Do not use a Ad-blockers (PiHole, Adguard-Home, Blocky, etc.) DNS server for your k8s nodes. Ad-blockers should be used on devices with a web browser. Remove any search domains from your hosts /etc/resolv.conf . Search domains have an issue with alpine based containers and DNS might not resolve in them. Do not disable ipv6, keep it enabled even if you aren't using it. Some applications will complain about ipv6 being disabled and logs will be spammed. Ensure you are using iptables in nf_tables mode. Enable packet forwarding on the hosts, and apply other sysctl tweaks: cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding = 1 fs.inotify.max_user_watches=65536 EOF sudo sysctl --system Make sure your nodes hostname appears in /etc/hosts , for example: 127.0.0.1 localhost 127.0.1.1 k8s-0 System \u00b6 For a trade-off in speed over security, disable AppArmor and Mitigations on Ubuntu: # /etc/default/grub GRUB_CMDLINE_LINUX=\"apparmor=0 mitigations=off\" and then reconfigure grub and reboot: sudo update-grub sudo reboot Setup unattended-upgrade for use with Kured to automatically patch and reboot your nodes. Disable swap","title":"Guidelines"},{"location":"installation/guidelines/#guidelines","text":"Here are several suggestions I have prior to installing Kubernetes or just general use. Some of these suggestions may only apply to Ubuntu.","title":"Guidelines"},{"location":"installation/guidelines/#storage","text":"Do not use NFS for application configuration data if that application uses SQLite with write ahead logging (WAL), or uses file locks. Applications like Sonarr, Radarr, Lidarr are clear examples to avoid using NFS for the configuration volume. Understand the importance, capabilities and limitations between file, block and object type storage.","title":"Storage"},{"location":"installation/guidelines/#networking","text":"Configure DNS on your nodes to use an upstream provider (e.g. 1.1.1.1 , 9.9.9.9 ), or your router's IP if you have DNS configured there and it's not pointing to a local adblocker DNS . Do not use a Ad-blockers (PiHole, Adguard-Home, Blocky, etc.) DNS server for your k8s nodes. Ad-blockers should be used on devices with a web browser. Remove any search domains from your hosts /etc/resolv.conf . Search domains have an issue with alpine based containers and DNS might not resolve in them. Do not disable ipv6, keep it enabled even if you aren't using it. Some applications will complain about ipv6 being disabled and logs will be spammed. Ensure you are using iptables in nf_tables mode. Enable packet forwarding on the hosts, and apply other sysctl tweaks: cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding = 1 fs.inotify.max_user_watches=65536 EOF sudo sysctl --system Make sure your nodes hostname appears in /etc/hosts , for example: 127.0.0.1 localhost 127.0.1.1 k8s-0","title":"Networking"},{"location":"installation/guidelines/#system","text":"For a trade-off in speed over security, disable AppArmor and Mitigations on Ubuntu: # /etc/default/grub GRUB_CMDLINE_LINUX=\"apparmor=0 mitigations=off\" and then reconfigure grub and reboot: sudo update-grub sudo reboot Setup unattended-upgrade for use with Kured to automatically patch and reboot your nodes. Disable swap","title":"System"},{"location":"installation/installing-kubernetes/","text":"Installing Kubernetes and Nodes \u00b6 Terraform will provide the infrastructer via the proxmox api. Nodes will be created and k3s will be bootstraped with k3sup. This cluster is a HA Cluster with k3s etcd backend. During the inital bootstrap kube-vip will be deployed as a daemonset for the ha k8s api.","title":"Installing Kubernetes"},{"location":"installation/installing-kubernetes/#installing-kubernetes-and-nodes","text":"Terraform will provide the infrastructer via the proxmox api. Nodes will be created and k3s will be bootstraped with k3sup. This cluster is a HA Cluster with k3s etcd backend. During the inital bootstrap kube-vip will be deployed as a daemonset for the ha k8s api.","title":"Installing Kubernetes and Nodes"},{"location":"installation/preparing-nodes/","text":"Preparing Nodes \u00b6 Install Proxmox \u00b6 Download Proxmox VE and flash it to a USB drive, boot the device from the USB drive and install Proxmox VE Configure Proxmox and the network \u00b6 !!! Use as Static IP for your Proxmox node. This makes it way more easy to create a cluster with new nodes. Prepare Ubuntu/Centos for k8s \u00b6 K8s Images are Build with Hashicorps Packer. With serveral configs which will prepare both os for automatic patching with kured. TODO - Add Link for Packer Repo \u00b6","title":"Preparing Nodes"},{"location":"installation/preparing-nodes/#preparing-nodes","text":"","title":"Preparing Nodes"},{"location":"installation/preparing-nodes/#install-proxmox","text":"Download Proxmox VE and flash it to a USB drive, boot the device from the USB drive and install Proxmox VE","title":"Install Proxmox"},{"location":"installation/preparing-nodes/#configure-proxmox-and-the-network","text":"!!! Use as Static IP for your Proxmox node. This makes it way more easy to create a cluster with new nodes.","title":"Configure Proxmox and the network"},{"location":"installation/preparing-nodes/#prepare-ubuntucentos-for-k8s","text":"K8s Images are Build with Hashicorps Packer. With serveral configs which will prepare both os for automatic patching with kured.","title":"Prepare Ubuntu/Centos for k8s"},{"location":"installation/preparing-nodes/#todo-add-link-for-packer-repo","text":"","title":"TODO - Add Link for Packer Repo"},{"location":"networking/dns/","text":"DNS \u00b6 My DNS setup may seem a bit complicated at first, but it allows for completely automatic management of DNS entries for service and ingress objects. Components \u00b6 Traefik \u00b6 Traefik is my cluster ingress controller. It is set to a externalIPs so that I can forward a port on my router directly to the service. CoreDNS with k8s_gateway \u00b6 CoreDNS is running on my OPNsense router. I have included the k8s_gateway plugin so that it can directly connect to my cluster and automatically serve DNS for all my ingresses. external-dns \u00b6 external-dns runs in my cluster and is connected to Cloudflare. When an ingress has the external-dns/is-public: \"true\" annotation set external-dns will add, update or delete that record in Cloudflare automatically Dynamic DNS \u00b6 My home IP can change at any given time and in order to keep my WAN IP address up to date on Cloudflare I have deployed a CronJob ( link ). This runs in my cluster and periodically checks and updates my A record of ipv4.domain.tld How it all works together \u00b6 When I am connected to my home network, my DNS server is set to Blocky . I have configured this to forward all requests for my own domain names to the CoreDNS instance that is running on my router. If an ingress or service exists for the requested address, k8s_gateway will respond with the IP address that it received from my cluster. If it doesn't exist, it will respond with NXDOMAIN . When I am outside my home network, I will probably use whatever DNS is provided to me. When I request an address for one of my domains, it will query my domains DNS server and will respond with the DNS record that was set by external-dns .","title":"DNS"},{"location":"networking/dns/#dns","text":"My DNS setup may seem a bit complicated at first, but it allows for completely automatic management of DNS entries for service and ingress objects.","title":"DNS"},{"location":"networking/dns/#components","text":"","title":"Components"},{"location":"networking/dns/#traefik","text":"Traefik is my cluster ingress controller. It is set to a externalIPs so that I can forward a port on my router directly to the service.","title":"Traefik"},{"location":"networking/dns/#coredns-with-k8s_gateway","text":"CoreDNS is running on my OPNsense router. I have included the k8s_gateway plugin so that it can directly connect to my cluster and automatically serve DNS for all my ingresses.","title":"CoreDNS with k8s_gateway"},{"location":"networking/dns/#external-dns","text":"external-dns runs in my cluster and is connected to Cloudflare. When an ingress has the external-dns/is-public: \"true\" annotation set external-dns will add, update or delete that record in Cloudflare automatically","title":"external-dns"},{"location":"networking/dns/#dynamic-dns","text":"My home IP can change at any given time and in order to keep my WAN IP address up to date on Cloudflare I have deployed a CronJob ( link ). This runs in my cluster and periodically checks and updates my A record of ipv4.domain.tld","title":"Dynamic DNS"},{"location":"networking/dns/#how-it-all-works-together","text":"When I am connected to my home network, my DNS server is set to Blocky . I have configured this to forward all requests for my own domain names to the CoreDNS instance that is running on my router. If an ingress or service exists for the requested address, k8s_gateway will respond with the IP address that it received from my cluster. If it doesn't exist, it will respond with NXDOMAIN . When I am outside my home network, I will probably use whatever DNS is provided to me. When I request an address for one of my domains, it will query my domains DNS server and will respond with the DNS record that was set by external-dns .","title":"How it all works together"},{"location":"networking/general/","text":"Networking \u00b6 Description of how my network is set up. Name CIDR Proxmox API 192.168.178.200/24 Kubernetes - API 192.168.178.10/24 k8s pods 10.10.0.0/16 k8s services 192.168.178.241/32 Running high-available control-plane \u00b6 In order to make the controle plane ha i used kube-vip since my own network dosent support bgp routing as of now. Exposing services via Ingress \u00b6 With no flat networking all services must be exposed via LoadBalancer or Ingress. As of now all services are exposed via traefik Mixed-protocol services \u00b6 I have enabled the MixedProtocolLBService=true feature-gate on my cluster. This means that I can combine the same port with different protocols (UDP and TCP) on the same service.","title":"General"},{"location":"networking/general/#networking","text":"Description of how my network is set up. Name CIDR Proxmox API 192.168.178.200/24 Kubernetes - API 192.168.178.10/24 k8s pods 10.10.0.0/16 k8s services 192.168.178.241/32","title":"Networking"},{"location":"networking/general/#running-high-available-control-plane","text":"In order to make the controle plane ha i used kube-vip since my own network dosent support bgp routing as of now.","title":"Running high-available control-plane"},{"location":"networking/general/#exposing-services-via-ingress","text":"With no flat networking all services must be exposed via LoadBalancer or Ingress. As of now all services are exposed via traefik","title":"Exposing services via Ingress"},{"location":"networking/general/#mixed-protocol-services","text":"I have enabled the MixedProtocolLBService=true feature-gate on my cluster. This means that I can combine the same port with different protocols (UDP and TCP) on the same service.","title":"Mixed-protocol services"},{"location":"storage/rook-pvc-backup/","text":"Manual Data Backup \u00b6 Create the toolbox container \u00b6 Ran from your workstation kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192.168.42.60:/Data /mnt/nfsdata Backup data to a NFS share \u00b6 Ran from your workstation Pause the Flux Helm Release flux suspend hr home-assistant -n home Scale the application down to zero pods kubectl scale deploy/home-assistant --replicas 0 -n home Get the csi-vol-* string kubectl get pv/(kubectl get pv | grep home-assistant-config-v1 | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data tar czvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data/ . umount /mnt/data rbd unmap -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa","title":"Manual Data Backup"},{"location":"storage/rook-pvc-backup/#manual-data-backup","text":"","title":"Manual Data Backup"},{"location":"storage/rook-pvc-backup/#create-the-toolbox-container","text":"Ran from your workstation kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192.168.42.60:/Data /mnt/nfsdata","title":"Create the toolbox container"},{"location":"storage/rook-pvc-backup/#backup-data-to-a-nfs-share","text":"Ran from your workstation Pause the Flux Helm Release flux suspend hr home-assistant -n home Scale the application down to zero pods kubectl scale deploy/home-assistant --replicas 0 -n home Get the csi-vol-* string kubectl get pv/(kubectl get pv | grep home-assistant-config-v1 | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data tar czvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data/ . umount /mnt/data rbd unmap -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa","title":"Backup data to a NFS share"},{"location":"storage/rook-pvc-restore/","text":"Manual Data Restore \u00b6 Create the toolbox container \u00b6 Ran from your workstation kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192.168.42.60:/Data /mnt/nfsdata Restore data from a NFS share \u00b6 Ran from your workstation Apply the PVC kubectl apply -f cluster/apps/home/home-assistant/config-pvc.yaml Get the csi-vol-* string kubectl get pv/(kubectl get pv | grep home-assistant-config-v1 | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I{} sh -c 'mkfs.ext4 {}' rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I{} mount {} /mnt/data tar xvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data umount /mnt/data rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212","title":"Manual Data Restore"},{"location":"storage/rook-pvc-restore/#manual-data-restore","text":"","title":"Manual Data Restore"},{"location":"storage/rook-pvc-restore/#create-the-toolbox-container","text":"Ran from your workstation kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192.168.42.60:/Data /mnt/nfsdata","title":"Create the toolbox container"},{"location":"storage/rook-pvc-restore/#restore-data-from-a-nfs-share","text":"Ran from your workstation Apply the PVC kubectl apply -f cluster/apps/home/home-assistant/config-pvc.yaml Get the csi-vol-* string kubectl get pv/(kubectl get pv | grep home-assistant-config-v1 | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I{} sh -c 'mkfs.ext4 {}' rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I{} mount {} /mnt/data tar xvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data umount /mnt/data rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212","title":"Restore data from a NFS share"}]}